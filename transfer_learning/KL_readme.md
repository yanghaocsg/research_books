# ç†è§£KLæ•£åº¦ ï¼ˆKL Divergence)

[TOC]

## 1 è§£å†³é—®é¢˜ï¼š æµ‹é‡ä¸¤ä¸ªåˆ†å¸ƒå·®å¼‚ï¼Œæ‰¾åˆ°å‚æ•°ï¼Œä½¿å·®å¼‚æœ€å°ï¼ˆKLæµ‹é‡,ç›¸åŒæ—¶KL=0ï¼‰ï¼Œå¦‚æœcosæµ‹é‡ï¼Œå·®å¼‚å€¼æœ€å¤§ï¼ˆç›¸åŒæ—¶cos =1)

-   KL Divergence is a measure of how one probability distribution (P) is different from a second probability distribution (Q).
-    If two distributions are identical, their KL div. should be 0. 
-   Hence, by minimizing KL div., we can find paramters of the second distribution (Q) that approximate P.
-   è¯´äººè¯ï¼š
    -   æµ‹é‡ä¸¤ä¸ªåˆ†å¸ƒï¼ˆP)(Qï¼‰å·®è·
    -   å¦‚æœä¸€è‡´ï¼ŒKL ä¸º0 ï¼ˆæœ€å°åŒ–ä¸ºç›®æ ‡ï¼‰
        -    å¦‚æœç”¨mseé‡åº¦ï¼Œæœ€å°ä¸ºç›®æ ‡ï¼›
        -   å¦‚æœç”¨cosé‡åº¦ï¼Œæœ€å¤§ä¸ºç›®æ ‡
    -   æ‰¾åˆ°æœ€åˆé€‚çš„å‚æ•°ï¼Œä½¿KLæœ€å° 
        -   æ ¸æ–¹æ³•ï¼Œç”¨çº¿æ€§ã€é«˜é˜¶æˆ–è€…rbfæ¥æ‹Ÿåˆé—®é¢˜
            -   æ‰¾åˆ°æå€¼
            -   æ‰¾åˆ°æå€¼æ‰€å¯¹åº”çš„å‚æ•°

## 2 å®éªŒ - ç”¨KLæ•£åº¦ï¼Œè¿›è¡Œä¸¤ä¸ªä¸€ç»´æ­£æ€åˆ†å¸ƒæ‹Ÿåˆ

## 2.1 æºåˆ†å¸ƒï¼ˆP)ï¼Œä¸¤ä¸ªæ­£æ€åˆ†å¸ƒä¹‹å’Œ

-   ~~~
    mu1,sigma1 = -5,1
    mu2,sigma2 = 10,1
    
    gaussian1 = torch.distributions.Normal(mu1,sigma1) 
    gaussian2 = torch.distributions.Normal(mu2,sigma2)
    ~~~

-   ![1581819523385](1581819523385.png)

-   ~~~
    # PX = gaussian1 + gaussian2
    
    x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)
    px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()
    ~~~

-   ![1581819606291](1581819606291.png)

## 2.2 ç›®æ ‡åˆ†å¸ƒï¼ˆQ)ï¼Œä¸€ä¸ªæ­£æ€åˆ†å¸ƒ

-   ~~~
    mu = torch.tensor([0.0])
    sigma = torch.tensor([1.0])
    
    plt.figure(figsize=(14,4))
    x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)
    Q = torch.distributions.Normal(mu,sigma) # this should approximate P, eventually :-)
    qx = Q.log_prob(x).exp()
    plt.subplot(1,2,2)
    plt.plot(x.numpy(),qx.detach().numpy())
    plt.title('$Q(X)$')
    ~~~

-   

 ![1581819676304](1581819676304.png)

## 2.3 ç”¨KLæ•£åº¦è¿›è¡ŒP,Qæ‹Ÿåˆï¼ˆå­¦ä¹ ï¼‰

-   KLæ•£åº¦

    -   $$ğ·_{ğ¾ğ¿}(ğ‘ƒ(ğ‘¥)||ğ‘„(ğ‘‹))=âˆ‘_{ğ‘¥âˆˆğ‘‹}ğ‘ƒ(ğ‘¥)log(ğ‘ƒ(ğ‘¥)/ğ‘„(ğ‘¥))$$

-   pytorch KLæ•£åº¦ä»£ç 

    -   https://pytorch.org/docs/stable/nn.html#torch.nn.functional.kl_div

-   ä»£ç 

    -   ~~~
        px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()
        qx = Q.log_prob(x).exp()
        F.kl_div(qx.log(),px)
        ~~~

    -   ~~~
        px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()
        qx = Q.log_prob(x)
        F.kl_div(qx,px)
        ---------------
        tensor(2.0812)  # D(P, Q) = 2.08
        ~~~

-   è¿­ä»£è®¡ç®—ï¼ˆkernelæ–¹æ³•)

    -   ~~~
        def optimize_loss(px: torch.tensor, loss_fn: str, muq: float = 0.0, sigmaq: float = 1.0,\
                          subsample_factor:int = 3,mode:str = 'min') -> Tuple[float,float,List[int]]:
            
            mu = torch.tensor([muq],requires_grad=True)
            sigma = torch.tensor([sigmaq],requires_grad=True)    
        
            opt = torch.optim.Adam([mu,sigma])
        
            loss_val = []
            epochs = 10000
        
            #required for animation
            all_qx,all_mu = [],[]
            subsample_factor = 3 #have to subsample to reduce memory usage
        
            torch_loss_fn = getattr(F,loss_fn)
            for i in range(epochs):
                Q = torch.distributions.Normal(mu,sigma) # this should approximate P
                if loss_fn!='kl_div': # we need to exponentiate q(x) for these and few other cases
                    qx = Q.log_prob(x).exp()
                    all_qx.append(qx.detach().numpy()[::subsample_factor])
                else:
                    qx = Q.log_prob(x)
                    all_qx.append(qx.exp().detach().numpy()[::subsample_factor])
                    
                if mode=='min':
                    loss = torch_loss_fn(qx,px)
                else:
                    loss = -torch_loss_fn(qx,px,dim=0)
            #   backward pass
                opt.zero_grad()
                loss.backward()    
                opt.step()
                loss_val.append(loss.detach().numpy())
                all_mu.append(mu.data.numpy()[0])
                
                
                if i%(epochs//10)==0:
                    print('Epoch:',i,'Loss:',loss.data.numpy(),'mu',mu.data.numpy()[0],'sigma',sigma.data.numpy()[0])
        
        
            print('Epoch:',i,'Loss:',loss.data.numpy(),'mu',mu.data.numpy()[0],'sigma',sigma.data.numpy()[0])
            
            plt.figure(figsize=(14,6))
            plt.subplot(2,2,1)
            plt.plot(loss_val)
            plt.xlabel('epoch')
            plt.ylabel(f'{loss_fn} (Loss)')
            plt.title(f'{loss_fn} vs epoch')
            
            plt.subplot(2,2,2)
            plt.plot(all_mu)
            plt.xlabel('epoch')
            plt.ylabel('$\mu$')
            plt.title('$\mu$ vs epoch')
            
            return mu.data.numpy()[0],sigma.data.numpy()[0],all_qx
        ~~~

    -   ~~~
        # ç”¨KLæ•£åº¦åšé‡åº¦ï¼Œinit(mu =0.0, sigma=1.0)
        x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)
        px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()
        mu,sigma,all_qx = optimize_loss(px, loss_fn='kl_div', muq = 0.0, sigmaq = 1.0)
        ~~~

    -   è¿­ä»£10000æ­¥ï¼Œ$Loss = 0.14,\ \mu = 2.49,\ \sigma = 5.2$

    -   ![1581820113250](1581820113250.png)

    -   åŠ¨ç”»å±•ç¤º

        -   ![1581820251374](1581820251374.png)
        -   ï¼[kl_div](kl_div.gif)

    ## 2.4 ç”¨MSEè¿›è¡Œæ‹Ÿåˆ

    -   mseæ‹Ÿåˆï¼Œ$\mu=0.0, \sigma=1$, æœ€ç»ˆæ‹Ÿåˆåˆ°å·¦è¾¹è¿™ä¸ªé«˜å³°

        -   ~~~
            x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)
            px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()
            mu,sigma,all_qx = optimize_loss(px, loss_fn='mse_loss', muq = 0.0, sigmaq = 1.0)
            ~~~

        -   ![1581820442238](1581820442238.png)

        -   

        -   ![1581820463248](1581820463248.png)

    -   mseæ‹Ÿåˆï¼Œ$\mu=5.0, \sigma=1.0$,æœ€ç»ˆæ‹Ÿåˆåˆ°å³è¾¹è¿™ä¸ªé«˜å³°

        -   ~~~
            x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)
            px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()
            mu,sigma,all_qx = optimize_loss(px, loss_fn='mse_loss', muq = 5.0, sigmaq = 1.0)
            
            fn = 'mse_loss_mean5.gif'
            ani = create_animation(x,px,all_qx,fn=fn)
            Image(f"./{fn}")
            ~~~

        -   ![1581820565400](1581820565400.png)

## 2.4 cos lossè¿›è¡Œæ‹Ÿåˆ

-   ~~~
    x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)
    px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()
    mu,sigma,all_qx = optimize_loss(px, loss_fn='cosine_similarity', muq = 5.0, sigmaq = 1.0,mode='max')
    
    fn = 'cosine_similarity.gif'
    ani = create_animation(x,px,all_qx,fn=fn)
    Image(f"./{fn}")
    ~~~

-   ![1581834358842](1581834358842.png)

-   æœ€ç»ˆæ‹Ÿåˆåˆ°å³è¾¹é«˜å³°